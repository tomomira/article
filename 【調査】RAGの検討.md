# 【調査】AWS BedrockにおけるRAG構成の検討

## Knowledge Base for Amazon Bedrockの利用

AWS BedrockでRAG（Retrieval Augmented Generation）を実装する際、Knowledge Base for Amazon Bedrockを利用する構成は、多くの場合において理想的な選択肢となります。

### 主な利点

- **開発の簡素化**:
    - データソース（S3）を指定するだけで、ドキュメントの分割、ベクトル化、ベクトルDBへの保存といった一連のパイプラインを自動で構築・管理します。
    - これにより、開発者は本来のアプリケーション開発に集中できます。
- **フルマネージド**:
    - インフラの運用・管理が不要です。
    - データソースとの同期も自動で行われ、ナレッジは常に最新の状態に保たれます。
- **シンプルなAPI利用**:
    - `RetrieveAndGenerate` APIを呼び出すだけで、関連情報の検索から回答生成までの一連の処理が完結します。
- **柔軟なベクトルDB選択**:
    - バックエンドのベクトルDBとして、Amazon OpenSearch Serverless, Pinecone, Redis Enterprise Cloudなどから選択可能です。

### ユースケース

迅速なRAG環境の構築と、運用負荷の低減を重視する場合に非常に有効です。
より細かいカスタマイズ（独自のチャンキング手法や特殊な埋め込みモデルの利用など）が必要な場合は、個別にパイプラインを構築することも選択肢となります。

---

## Knowledge Baseにおける埋め込みモデル（Embedding Model）の利点

Knowledge Baseで埋め込みモデルを選択できることには、「検索精度」「コスト」「対応言語」の3つの観点から大きな利点があります。

### 1. 検索精度の最適化

- **モデルの得意分野**: 各モデルは異なるデータで学習されており、特定のドメインや文脈の理解度が異なります。ドキュメントの特性に合ったモデルを選ぶことで、検索精度が向上します。
- **回答品質への貢献**: 検索精度が向上すると、LLMがより適切なコンテキスト情報を利用できるため、最終的な回答の質も向上します。

### 2. コストの最適化

- **モデルごとの料金**: 埋め込み処理にはモデルごとに異なる料金が設定されています。
- **性能とコストのバランス**: ドキュメントの量や更新頻度、求められる精度に応じて、コストパフォーマンスに優れたモデルを選択できます。

### 3. 多言語対応

- **言語への特化**: 扱うドキュメントの言語に対応したモデルを選択することが不可欠です。
- **例**: 日本語のドキュメントを主に使用する場合、多言語対応モデル（例：Amazon Titan Text Embeddings G1 - Multilingual）を選択することで、日本語のニュアンスをより正確に捉えることが期待できます。

---

## 埋め込みモデル以外の主要なコンポーネント

Knowledge Baseにおいて、テキストのベクトル化には埋め込みモデルの選択が必須ですが、RAGプロセス全体では他にも重要な選択要素があります。

### 1. 生成AIモデル（Foundation Model）

- **役割**: Knowledge Baseが検索した情報を基に、**最終的な回答文を生成**します。
- **選択**: `RetrieveAndGenerate` APIを利用する際に、Anthropic ClaudeやLlama 2などの基盤モデルから選択します。
- **役割分担**:
    - **埋め込みモデル**: 関連情報を「見つける」専門家。
    - **生成AIモデル**: 見つかった情報から、自然で分かりやすい文章を「作る」専門家。

### 2. ベクトルデータベース（Vector Database）

- **役割**: 埋め込みモデルが生成したベクトルを格納し、高速な検索を実現します。
- **選択**: Amazon OpenSearch Serverless, Pinecone, Redis Enterprise Cloudなどから選択可能です。

### まとめ

Knowledge Baseのセットアップ時には**「埋め込みモデル」**と**「ベクトルデータベース」**を設定し、RAGの実行（APIコール）時には**「生成AIモデル」**を指定するという流れになります。

---

## RAG関連キーワード網羅リスト

### I. 基本概念・アーキテクチャ

*   **RAG (Retrieval Augmented Generation / 検索拡張生成)**
    *   **意味**: LLMが外部の知識源（ナレッジベース）を検索し、その結果を基に回答を生成する技術。LLMの知識を動的に拡張し、回答の正確性と信頼性を高める。

*   **ナレッジベース (Knowledge Base)**
    *   **意味**: RAGが検索対象とする情報の集まり。社内ドキュメント、マニュアル、FAQなど、回答の根拠としたいドキュメント群。

### II. データ準備・前処理 (Data Preparation / Preprocessing)

*   **チャンキング (Chunking)**
    *   **意味**: 長いドキュメントを、意味のある単位で小さな塊（チャンク）に分割すること。検索効率と精度を向上させるための重要な前処理。

*   **チャンクサイズ (Chunk Size)**
    *   **意味**: 1つのチャンクに含まれる文字数や単語数。このサイズが検索の精度やLLMに渡す情報量に影響を与える。

*   **チャンクオーバーラップ (Chunk Overlap)**
    *   **意味**: チャンクを分割する際に、隣接するチャンク間で内容を意図的に重複させること。文脈が途切れてしまい、重要な情報が失われるのを防ぐ効果がある。

*   **メタデータ (Metadata)**
    *   **意味**: 各チャンクに付与される付加情報（例：元のファイル名、作成日、カテゴリ、章タイトルなど）。検索結果のフィルタリングや、出典の提示に利用される。

### III. ベクトル化と検索 (Vectorization and Retrieval)

*   **ベクトル (Vector)**
    *   **意味**: テキストの意味的な特徴を捉えた数値の配列。

*   **埋め込みモデル (Embedding Model)**
    *   **意味**: テキストをベクトルに変換するAIモデル。

*   **ベクトルデータベース (Vector Database)**
    *   **意味**: ベクトルを格納し、類似度に基づいた高速な検索（ベクトル検索）を行うためのデータベース。

*   **ベクトル検索 / 類似度検索 (Vector Search / Similarity Search)**
    *   **意味**: 質問のベクトルと、データベース内の各チャンクのベクトルとの「近さ（類似度）」を計算し、最も関連性の高いチャンクを見つけ出す検索手法。

*   **ハイブリッド検索 (Hybrid Search)**
    *   **意味**: ベクトル検索と、従来のキーワード検索（全文検索）を組み合わせた検索手法。意味的な関連性とキーワードの一致度の両方を考慮するため、単独の検索手法よりも高い精度が期待できる。

*   **キーワード検索 / BM25 (Keyword Search / BM25)**
    *   **意味**: テキストに含まれる単語の出現頻度や重要度に基づいて関連性をスコアリングする古典的な検索アルゴリズム。ベクトル検索が苦手とする固有名詞や専門用語の検索に強い。

*   **再ランキング (Re-ranking)**
    *   **意味**: 検索システムが一度取得した検索結果（例：上位50件）を、より高度で計算コストの高いモデル（クロスエンコーダーなど）を使って、再度関連性の高い順に並べ替える処理。検索の最終的な精度を高めるために行われる。

### IV. 回答生成とプロンプト (Generation and Prompting)

*   **グラウンディング (Grounding)**
    *   **意味**: LLMが回答を生成する際に、提供された検索結果（コンテキスト）のみに基づいて回答するように制約すること。事実に基づいた回答を徹底させ、ハルシネーションを防ぐための重要な概念。

*   **ハルシネーション (Hallucination)**
    *   **意味**: LLMが、事実に基づかない、もっともらしい嘘の情報を生成してしまう現象。RAGはこれを抑制するための主要な対策の一つ。

*   **コンテキストウィンドウ (Context Window)**
    *   **意味**: LLMが一度に処理できるテキストの最大長（トークン数）。RAGでは、検索結果をこのウィンドウ内に収まるように整形してLLMに渡す必要がある。

*   **引用 / 出典提示 (Citation / Source Attribution)**
    *   **意味**: 生成された回答が、ナレッジベース内のどのドキュメントのどの部分を根拠にしているかを示すこと。回答の信頼性を高め、ユーザーが元情報を確認できるようにするために不可欠。

### V. 評価とフレームワーク (Evaluation and Frameworks)

*   **LangChain / LlamaIndex**
    *   **意味**: RAGをはじめとするLLMアプリケーションを効率的に開発するための主要なフレームワーク。データ接続、変換、検索、生成といった一連の処理をコンポーネントとして提供する。

*   **RAGAS (Retrieval-Augmented Generation Assessment)**
    *   **意味**: RAGシステムの性能を評価するためのフレームワークの一つ。検索の性能（リトリーバル）と生成の品質（ジェネレーション）を多角的に評価する指標を提供する。

*   **忠実性 (Faithfulness)**
    *   **意味**: 生成された回答が、提供されたコンテキストにどれだけ忠実であるかを示す評価指標。ハルシネーションの度合いを測る。

*   **回答の関連性 (Answer Relevancy)**
    *   **意味**: 生成された回答が、ユーザーの質問にどれだけ的確に答えているかを示す評価指標。

### VI. LLM生成パラメータ (LLM Generation Parameters)

*   **temperature (温度)**
    *   **意味**: 生成されるテキストのランダム性を調整する値。

*   **Top-K**
    *   **意味**: 次の単語予測で、確率上位K個の候補から単語を選択する手法。

*   **Top-P (Nucleus Sampling)**
    *   **意味**: 次の単語予測で、確率の合計がPを超えるまでの候補群から単語を選択する手法。
